# LLM Token-Based Rate Limiting

LLM Token-Based Rate Limiting is designed to optimize the API usage when interacting with self-hosted large language models or LLMs from cloud providers.
The system allows you to manage the resource consumption efficiently, ensuring reliable experience for users while preventing overuse and potential service disruption.

## Key Concepts

## Token

- Tokens: A token is a unit of measure that represents the cost of processing a request to LLM.
The total number of tokens consumed for a session or user can be tracked to enforce rate limits.
  * `total_tokens`: Represents the total number of tokens, including both the prompt tokens and generated completion toknes.
  * `prompt_tokens`: Represents the tokens provided by the user as input to the prompt.
  * `completion_tokens`: Represents the tokens generated by LLM in response to the prompt.

## Rate Limiting Levels

- Global Rate Limit: A total cap on the number of tokens that can be consumed across all users and requests within a defined window (e.g 1,000 tokens per minute).
- User-level Rate Limit: Specific limits set for individual user to ensure high-demand users do not monopolize resources.
- Model-level Rate Limit: Limits applied to specific model to control the cost for expensive models.


## Local Development

1. Setup Kind Cluster(enable Metal LB)
   ```shell
   kind create cluster --name envoy-gateway
   ```

2. Install EG
   ```shell
   helm install eg oci://docker.io/envoyproxy/gateway-helm --version v0.0.0-latest -n envoy-gateway-system --create-namespace
   ```
3. Install AIG
   ```shell
   docker build . -t ghcr.io/tetratelabs/ai-gateway-extproc --build-arg NAME=extproc && docker build . -t ghcr.io/tetratelabs/ai-gateway-controller --build-arg NAME=controller
   kind load docker-image ghcr.io/tetratelabs/ai-gateway-extproc:latest --name envoy-gateway | kind load docker-image ghcr.io/tetratelabs/ai-gateway-controller:latest --name envoy-gateway
   kubectl apply -f manifests/charts/ai-gateway-helm/crds
   helm install aig manifests/charts/ai-gateway-helm -n ai-gateway-system --create-namespace
   ```
4. Install RateLimit Server
   ```shell
   kubectl apply -f ./test/e2e/init/ratelimit
   ```
5. Update EG config
   ```shell
   kubectl apply -f ./test/e2e/init/envoygateway
   kubectl rollout restart -n envoy-gateway-system deployment envoy-gateway
   ```
6. Apply LLMRoute

```shell
kubectl apply -f - <<EOF
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: LLMRoute
metadata:
  name: ratelimit-demo
  namespace: default
spec:
  targetRefs:
    - name: ai-eg
      kind: Gateway
      group: gateway.networking.k8s.io
  backends:
    - backendRef:
        name: huggingface-backend-ratelimit
        kind: Backend
        group: gateway.envoyproxy.io
      trafficPolicy:
        rateLimit:
          rules:
          - headers:
            - name: x-ai-gateway-llm-model-name
              type: Exact
              value: hf-opt-125m-chat
            - name: x-user-id
              type: Distinct
            limits:
            - type: Token
              quantity: 10
---
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: Backend
metadata:
  name: huggingface-backend-ratelimit
  namespace: default
spec:
  endpoints:
    - fqdn:
        hostname: huggingface-backend.default.svc.cluster.local
        port: 3000
---
apiVersion: gateway.networking.k8s.io/v1
kind: GatewayClass
metadata:
  name: eg
spec:
  controllerName: gateway.envoyproxy.io/gatewayclass-controller
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: ai-eg
spec:
  gatewayClassName: eg
  listeners:
    - name: http
      protocol: HTTP
      port: 80
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: huggingface-backend
---
apiVersion: v1
kind: Service
metadata:
  name: huggingface-backend
  labels:
    app: huggingface-opt
    service: huggingface-backend
spec:
  ports:
    - name: http
      port: 3000
      targetPort: 8080
  selector:
    app: huggingface-opt
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: huggingface-opt
  name: huggingface-opt
spec:
  replicas: 1
  selector:
    matchLabels:
      app: huggingface-opt
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: huggingface-opt
    spec:
      serviceAccountName: huggingface-backend
      containers:
      - args:
        - --model_id=facebook/opt-125m
        - --model_name=hf-opt-125m-chat
        - --backend=huggingface
        image: kserve/huggingfaceserver
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 3
          httpGet:
            path: /openai/v1/models/hf-opt-125m-chat
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        name: huggingface-openai
        ports:
        - containerPort: 8080
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /openai/v1/models/hf-opt-125m-chat
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 5
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 1
            memory: 2Gi
---
EOF
```

7. Send an inference request

Export Gateway host:

If you have an external load balancer like MetalLB, you can get the external IP address of the gateway by running the following command:

```shell
, you can get the external IP address of the gateway by running the following command:

```shell
export GATEWAY_HOST=$(kubectl get gateway/ai-eg -o jsonpath='{.status.addresses[0].value}')
```

Or you can port-forward the envoy service to localhost:

```shell
export ENVOY_SERVICE=$(kubectl get svc -n envoy-gateway-system --selector=gateway.envoyproxy.io/owning-gateway-namespace=default,gateway.envoyproxy.io/owning-gateway-name=ai-eg -o jsonpath='{.items[0].metadata.name}')

kubectl -n envoy-gateway-system port-forward service/${ENVOY_SERVICE} 8888:80 &

export GATEWAY_HOST=localhost:8888
```

Send an OpenAI request to chat completion endpoint of the model
```shell
curl -X POST $GATEWAY_HOST/openai/v1/chat/completions -H "content-type: application/json" -H "x-ai-gateway-llm-backend: huggingface-backend-ratelimit" -H "x-user-id: test1" -d '{"model": "hf-opt-125m-chat", "messages": [{"role":"system","content":"You are a helpful assistant. Please answer all of my questions in pirate speak, matey"}, {"role": "user","content": "why open source is important?"}], "stream":false, "max_tokens" : 10}'

{"id":"5aaa42ad-7b1e-43e5-a2c9-0f8b2e13cfbe","choices":[{"finish_reason":"length","index":0,"message":{"content":"I'm not sure if this is a good idea","tool_calls":null,"role":"assistant","function_call":null},"logprobs":null}],"created":1727074495,"model":"hf-opt-125m-chat","system_fingerprint":null,"object":"chat.completion","usage":{"completion_tokens":10,"prompt_tokens":27,"total_tokens":37}}
```
